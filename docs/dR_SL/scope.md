[]{#_Toc46119879 .anchor}

[]{#_Toc423356090 .anchor}Contents

[Scope Statement 1](#_Toc423356088)

[Declustered RAIDZ 1](#_Toc423356089)

[Contents 3](#_Toc423356090)

[Revision History 5](#_Toc423356091)

[1. Milestone Overview 6](#milestone-overview)

[1.1. Declustered RAIDZ Scope Statement
6](#declustered-raidz-scope-statement)

[1.2. Terms 6](#terms)

[1.3. Introduction 6](#introduction)

[2. Problem Statement 8](#problem-statement)

[3. Project Goals 9](#project-goals)

[4. In Scope 10](#in-scope)

[5. Out of Scope 12](#out-of-scope)

[6. Project Assumptions 13](#project-assumptions)

[7. Key Deliverables 14](#key-deliverables)

[8. Landing Target 15](#landing-target)

[9. Key Milestones 16](#key-milestones)

***\
***

***Figures***

**No table of figures entries found.**

[]{#_Toc423356091 .anchor}Revision History

  ---------- ------------------------------------------ ------------ -------------
  Revision   Description                                Date         Author
  0.1        Final draft                                2015-06-23   Isaac Huang
  1.1        Revisions following Argonne review         2015-08-21   Isaac Huang
  1.2        Final revisions following Argonne review   2015-09-09   Isaac Huang
  ---------- ------------------------------------------ ------------ -------------

[[[[]{#_Ref46905290 .anchor}]{#_Ref46905265 .anchor}]{#_Toc46134343
.anchor}]{#_Toc82841952 .anchor}**Author: Isaac Huang**

**Contributors: Eric Barton, John Salinas, Don Brady, John Carrier**

Milestone Overview 
===================

Declustered RAIDZ Scope Statement
---------------------------------

The following scope statement applies to the CORAL project within
the CFS Aggregate Scalable Storage Unit Performance as defined by MS6 in
the NRE Contract.

MS6 Description:

The Subcontractor shall develop parity declustering and distributed
sparing for RAIDZ. The Subcontractor shall implement this by providing
new pseudo-random data and parity mapping schemes for RAIDZ, RAIDZ2 and
RAIDZ3 storage pools. Although creating a separate parity declustered
RAIDZ VDEV driver may minimize changes to the current RAIDZ VDEV and,
hence, ease adoption and integration into the ZFS on Linux upstream,
this decision can be safely delayed until the beginning of the
implementation stage.

The Subcontractor shall extend the ZFS monitoring and administration
utilities to manage these new pools and provide policies to trigger
automatic recovery and throttle performance impact of recovery and
rebalancing on application I/O. I/O load during recovery will therefore
be distributed to all drives and so can be throttled back to minimize
impact on application I/O while keeping recovery time within acceptable
limits.

The Subcontractor shall provide an analysis of the I/O performance
obtained from RAIDZ with declustered parity using the same SSU (or
similar) to that used on the Argonne Aurora system as well as an
analysis of the impact of RAIDZ on MTTDL for scenarios with multiple
disk failures, as compared to original proposed solution. The
Subcontractor shall provide an analysis of the rebuild rate of the
parity declustered RAIDZ implementation under both idle I/O load and
heavy I/O load.

The Subcontractor shall provide all source changes to the ZFS upstream
source repository.

The Subcontractor shall deliver a Scope Statement that documents the
goals to be satisfied and specifies in-scope and out-of-scope elements
of work, assumptions and constraints and the key deliverables and
development milestones. Following completion of the Scope Statement, the
Subcontractor shall deliver a Solution Architecture that documents the
detailed solution requirements and outlines the solution broken down by
subsystem.

Terms
-----

The following terms are used throughout the document:

-   RAIDZ: a generic term to refer to ZFS RAIDZ1, RAIDZ2, RAIDZ3, and
    mirror, when there's no need to distinguish between them. Otherwise
    the more specific terms are used. The generic term RAID is also used
    when there's no need to distinguish between traditional RAID and
    RAIDZ.

-   Spare blocks: spare space distributed among all disks in a RAIDZ
    group.

-   Resilver: the process of reconstructing data/parity on a failed disk
    to spare blocks in a RAIDZ group.

-   Spare rebalance: the process of copying reconstructed data/parity
    from previous spare blocks to a replacement disk so that distributed
    spare blocks become available again.

-   COW: copy on write. ZFS never writes in place.

-   ZED: ZFS Event Daemon monitors events generated by the ZFS
    kernel module. When a zevent (ZFS Event) is posted, ZED will run any
    ZEDLETs (ZFS Event Daemon Linkage for Executable Tasks) that have
    been enabled for the corresponding zevent class

Introduction
------------

In large-scale storage configurations needed to meet the IO requirements
of future HPC systems, disk failures are inevitable and must be viewed
as a normal incident rather than an exceptional event.

When disk failures happen, it is important that RAID parity
reconstruction completes as quickly as possible. Shorter rebuild times
significantly reduce exposure to multiple cascading disk failures, which
would lead to data loss. It's also important that RAID rebuilding
process minimally impacts the application IO. A drop in performance
would cause applications to run longer or even fail to complete in their
allotted time window.

The process of rebuilding a “traditional” RAID array, when replacing a
failing/failed disk by a new one, consists of reading all the data on
all the surviving disks in the array, reconstructing the original data
of the replaced disk, and writing it to the replacement disk. After this
process is complete, the array is restored to its original full
redundancy. In ZFS, there is a similar and equivalent process, called
resilvering, which is implemented differently as volume management is a
built-in part of ZFS. This process starts by traversing the ZFS block
pointer tree to discover all the blocks of the ZFS pool that were
affected by the replaced disk. Upon reaching one of these blocks, it is
read, reconstructed if necessary from the redundant/parity information,
its checksum is verified and the missing data or parity from the
replaced disk is written to the new disk.

The speed of rebuilding or resilvering is bounded by the write
throughput of a single replacement disk, thus the total time would grow
at least linearly (often much worse) with drive capacity. As drive
capacity continues to grow with little increase in throughput, rebuild
time can increase significantly. On large drives it can range from 10+
hours for an idle system, to multiple days or a week for an active
system. Since idle time is rare, a drive failure and subsequent rebuild
process can significantly affect a system’s performance.

Parity declustered RAID distributes data, parity information, and spare
capacity across all disks in a pool, in a way such that all of them
would participate in the rebuild process nearly evenly - i.e. all disks
will be read from and written to nearly evenly during any interval of
time in the rebuild process. Therefore the bottleneck in hardware is
eliminated.

Problem Statement
=================

Unlike traditional RAID, ZFS RAIDZ resilvering algorithm only
reconstructs used blocks on a storage pool. However, in practice for
pools nearly full, ZFS RAIDZ resilvering can be significantly slower
than traditional RAID rebuild:

1.  Dedicated spare disk limits the total write throughput.

2.  Before actual block reconstruction, ZFS must traverse the meta-data
    tree to find out where the blocks are. For an aged pool, due to the
    COW nature, the meta-data tree can be highly fragmented.

3.  Blocks are reconstructed in the order in which they are encountered
    during meta-data tree traversal. As a result, the IO pattern often
    tends to be random.

In comparison, traditional RAID rebuild does not need to read additional
meta-data, and generates sequential IO. The declustered ZFS RAIDZ
removes the bottleneck in hardware by:

1.  Removing the restriction that the number of disks in a RAIDZ group
    must equal the sum of data disks and parity disks in a redundancy
    group, thus enabling larger RAIDZ configuration.

2.  Distributing data, parity information, and spare capacity
    evenly across all disks in a RAIDZ group, so that resilvering IO is
    balanced among all surviving disks.

In addition, with large 16M block allocations, declustered RAIDZ makes
sure that each disk receives a contiguous chunk of 2M, which increases
resilvering efficiency by amortizing the seek cost of random IO.

Project Goals
=============

In order to reduce resilver time while minimizing impact on application
IO, the ZFS parity declustered RAIDZ will include the following
components:

-   A parity declustering scheme that distributes data, parity, and
    spare blocks nearly evenly across all disks, such that during any
    interval of time in the resilvering process every disk is read from
    and written to nearly evenly, in terms of both IOPS and bandwidth.

-   File blocks are split into contiguous data/parity blocks – e.g. for
    RAIDZ2 8+2, a 16Mb block is split into contiguous 2Mb disk blocks.

-   Representation of distributed spare blocks as logical spare disk, so
    that the resilver algorithm can reconstruct blocks on a failed disk
    to the spare space.

-   A spare-rebalance mechanism to restore distributed spare space by
    copying blocks to a replacement disk.

-   If necessary, increasing parallelism of the resilver algorithm to
    ensure all available disk capability can be used to recover
    redundancy on disk failure.

-   Dynamic throttle of resilvering IO to limit impact on application
    IO according to the severity of failures – e.g. minimal application
    impact on first disk failure, minimum rebuild time on 2^nd^ disk
    failure.

In Scope
========

RAIDZ 
------

-   The solution will not be specific to any particular vendor hardware.
    The following items are within the scope of this project:

-   Declustered RAIDZ mapping scheme

-   Ensure file blocks are split into contiguous data/parity disk blocks
    – e.g. for 8+2 declustered RAIDZ2, the largest file block of 16Mb
    should map to 10 2Mb disk blocks.

-   Parity blocks should be evenly distributed to avoid any hot spots.

-   Spare capacity should be evenly distributed.

-   All current RAIDZ redundancy schemes are supported.

-   Specification of supported disk configurations (to be documented in
    the HLD).

-   Dirty Time Logging (DTL) support for declustered RAIDZ groups to
    recover from transient disk outages.

-   If necessary, modifications of the resilver algorithm to enable full
    and efficient use of available disk capability.

-   Dynamic throttling of resilvering/rebalance IO

-   Administrative controls to configure resilvering speed and maximum
    impact on application IO according to fault severity.

-   Resilvering/rebalance should be able to make full use of available
    idle IO bandwidth not required by application workloads.

-   Priority of in-progress resilvering should be escalated in the event
    of additional drive failure.

-   Spare rebalance support to restore distributed spare space by
    copying blocks to a replacement disk.

-   In progress spare rebalance should be preempted by resilvering in
    the event of drive failure.

-   Modifications to the ZFS Event Daemon (ZED) to support declustered
    RAIDZ

-   Prompt initiation of resilvering when disk failure is detected and
    diagnosed.

-   Correct selection of distributed spare devices.

Test Plan 
----------

-   Unit tests to verify code in an automated continuous integration
    environment.

-   Integration tests to verify functionality with other related
    software components such as Lustre and CentOS (which Cray plans to
    use for its Pinehurst Lustre servers) on both an
    automated continuous integration environment and a live test
    cluster environment.  Tests will be automated to run through many
    different configurations on real hardware. 

-   System tests to verify the software works with the other CORAL
    software including function shipping, IO libraries and CPPR using a
    black box methodology. These will also include long running
    stability tests to stress the system and inject faults to find
    corner cases.  

Out of Scope
============

The following items are out of the scope of this project because the
block layout for data and distributed hot spare space is configured when
the VDEV is created and cannot be changed afterwards without changing
block mappings, which is itself out of scope:

-   Support of arbitrary disk configurations.\
    (Requirements for supported disk configurations will be documented
    in the HLD.)

-   Changing disk configuration (i.e. number of drives, and drive
    capacity) after declustered group has been created.

-   Changing redundancy level after declustered group has been created.

-   Sharing spare capacity between different declustered groups.\
    (Distributed spare space is local to a declustered RAIDZ and cannot
    be used in other VDEVs.)

    -   Note that the support of dedicated hot spare drives is not
        changed. They can be used with declustered VDEVs, shared among
        VDEVs and even different pools. They can be added/removed after
        pool creation.

-   Modifying spare capacity after declustered group has been created.
    Spare space cannot be added after creation of the declustered group.

-   Converting a non-declustered RAIDZ group to a declustered RAIDZ
    group.

 

Project Assumptions
===================

All the drives in the declustered group must be identical in performance
and capacity. Any deviation will result in reduced performance and/or
capacity based on the lowest sized drive in the array. Replacement
drives can have no less capacity than other drives in the array. Any
additional capacity will not be used.

Key Deliverables
================

-   Please see the CORAL SOW
    (B609815\_Intel\_NRE\_SOW\_v7.8\_redline-20150219-1124) for all
    milestones

Landing Target
==============

Development will be done on a CORAL branch on a ZFS/SPL repository on
HPDD’s git server. All completed work will be upstreamed to the ZFS on
Linux project and/or to Lustre, as appropriate

Key Milestones
==============

-   Please see the CORAL SOW
    (B609815\_Intel\_NRE\_SOW\_v7.8\_redline-20150219-1124) for all
    milestones


